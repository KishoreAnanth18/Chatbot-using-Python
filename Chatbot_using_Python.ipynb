{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KishoreAnanth18/Chatbot-using-Python/blob/main/Chatbot_using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awxyJwad5iFK"
      },
      "source": [
        "**CHATBOT USING PYTHON**      \n",
        "KISHORE ANANTH N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa5QM4xq5o-I"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoblOlQP4uKd"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAS5hMnU50qy"
      },
      "source": [
        "Importing and Reading the dataset(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDCe78iZ5DFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2c508d8-81ef-498c-8c13-281a0ba14dce"
      },
      "source": [
        "f = open('chatbot.txt','r',errors = 'ignore')\n",
        "raw_doc = f.read()\n",
        "raw_doc = raw_doc.lower()\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc)\n",
        "word_tokens = nltk.word_tokenize(raw_doc)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeuXYbcx56oD"
      },
      "source": [
        "Sentance token: Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn9c3R2lLgoo",
        "outputId": "f9f754c8-158b-4973-d107-b196908defc6"
      },
      "source": [
        "sent_tokens[:2]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"machine learning (ml) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks.\",\n",
              " 'it is seen as a part of artificial intelligence.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhJg_08T6BUE"
      },
      "source": [
        "Word token: Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2y3JxBTLr6V",
        "outputId": "74a3992f-6bb4-424a-b411-f1468e00c5a3"
      },
      "source": [
        "word_tokens[:2]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['machine', 'learning']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8IwtPHR6G8h"
      },
      "source": [
        "Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGladVtcLwTP"
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord (punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBlXEsAL6Ldp"
      },
      "source": [
        "Defining the Greeting function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfYi_GJetjD_"
      },
      "source": [
        "GREET_INPUTS = (\"hello\", \"hi\", \"greetings\", \"whatsup\", \"hey\", \"morning\", \"afternoon\", \"evng\")\n",
        "GREET_RESPONSES = (\"hi\",\"heyy\",\"hi there\", \"hello\", \"I am glad!\")\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLEY-5fa6avp"
      },
      "source": [
        "Response generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7t2zYdxvmvh"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBAHM-FDxBxz"
      },
      "source": [
        "def response (user_response):\n",
        "  robo1_response = ''\n",
        "  TfidVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english') \n",
        "  tfidf = TfidVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf == 0):\n",
        "    robo1_response = robo1_response + \"I am sorry I don't understand you!\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response + sent_tokens[idx]\n",
        "    return robo1_response"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkDTbrGH6hwK"
      },
      "source": [
        "Defining conversation start/end protocols"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhc8TAhgzFR6",
        "outputId": "ee643881-b181-4946-f5af-f1bb313706a4"
      },
      "source": [
        "flag = True\n",
        "print(\"BOT: I'm your buddy, Let's have a conversation! Also, if you want to end the chat, just type bye!\")\n",
        "while(flag == True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response!='bye'):\n",
        "    if(user_response == 'thanks' or user_response == 'thank you'):\n",
        "      flag = False\n",
        "      print(\"BOT: You are welcome!\")\n",
        "    else:\n",
        "      if(greet(user_response)!= None):\n",
        "        print(\"BOT: \" + greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_tokens))\n",
        "        print(\"BOT: \", end = \"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag = False\n",
        "    print(\"BOT: Goodbye!! Have a nice day!\")\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT: I'm your buddy, Let's have a conversation! Also, if you want to end the chat, just type bye!\n",
            "Hello\n",
            "BOT: heyy\n",
            "what is machine learning?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT: a representative book on research into machine learning during the 1960s was nilsson's book on learning machines, dealing mostly with machine learning for pattern classification.\n",
            "Tom M. Mitchell\n",
            "BOT: tom m. mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"a computer program is said to learn from experience e with respect to some class of tasks t and performance measure p if its performance at tasks in t, as measured by p, improves with experience e.\"this definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms.\n",
            "Machine learning (ML)\n",
            "BOT: as of 2020, many sources continue to assert that ml remains a subfield of ai.others have the view that not all ml is part of ai, but only an 'intelligent subset' of ml should be considered ai.\n",
            "bye\n",
            "BOT: Goodbye!! Have a nice day!\n"
          ]
        }
      ]
    }
  ]
}